name: Build

on:
  push:
    branches:
      - main
      - stage
      - dev
    paths:
      - "infra/core/**"
      - ".github/workflows/infra.yaml"
      - "services/**"
  pull_request:
    branches:
      - dev
      - test
    paths:
      - "infra/core/**"
      - ".github/workflows/infra.yaml"
      - "services/**"
  workflow_dispatch:

jobs:
  core_infra:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Extract variables
        id: vars
        run: |
          echo "app_name=$(grep '^app_name' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "aws_region=$(grep '^aws_region' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT

      - name: Set Region
        run: |
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: Set NEO4J Credentials
        id: neo4j-creds
        run: |
          echo "${{steps.vars.outputs.aws_region}}"
          credentials_arn=""
          if [ "${{ env.ENV_NAME }}" == "dev" ]; then
            credentials_arn="${{ secrets.AWS_DEV_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "prod" ]; then
            credentials_arn="${{ secrets.AWS_PROD_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "stage" ]; then
            credentials_arn="${{ secrets.AWS_STAGE_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "test" ]; then
            credentials_arn="${{ secrets.AWS_TEST_NEO4J_CREDENTIALS }}"
          fi
          echo "NEO4J_CREDENTIALS_ARN=$credentials_arn" >> $GITHUB_OUTPUT

      - name: Get neo4j secrets
        uses: aws-actions/aws-secretsmanager-get-secrets@v1
        with:
          secret-ids: |
            NEO4J_CREDS, ${{steps.neo4j-creds.outputs.NEO4J_CREDENTIALS_ARN}}
          parse-json-secrets: true

      - name: Set up Python ${{matrix.python-version}}
        uses: actions/setup-python@v4
        with:
          python-version: ${{matrix.python-version}}

      - name: Build data management layer package
        id: build-data-management-layer
        run: |
          mkdir -p build/services_layer/python/models
          cp services/services_layer/src/requirements.txt build/services_layer/python/
          cp -r services/services_layer/src/services_layer/* build/services_layer/python/
          pip install -r build/services_layer/python/requirements.txt -t build/services_layer/python/

      - name: Build Fetch Daily Summaries Lambda Function
        id: build-fetch-daily-summaries
        run: |
          mkdir -p build/fetch_daily_summaries
          cp services/fetch_daily_summaries/src/requirements.txt build/fetch_daily_summaries/
          cp services/fetch_daily_summaries/src/fetch_daily_summaries/lambda_handler.py build/fetch_daily_summaries/
          pip install -r build/fetch_daily_summaries/requirements.txt -t build/fetch_daily_summaries/

      - name: Build Parse arXiv Summaries Lambda Function
        id: build-parse-arxiv-summaries
        run: |
          mkdir -p build/parse_arxiv_summaries
          cp services/parse_arxiv_summaries/src/requirements.txt build/parse_arxiv_summaries/
          cp services/parse_arxiv_summaries/src/parse_arxiv_summaries/lambda_handler.py build/parse_arxiv_summaries/
          pip install -r build/parse_arxiv_summaries/requirements.txt -t build/parse_arxiv_summaries/

      - name: Build Store arXiv Summaries Lambda Function
        id: build-store-arxiv-summaries
        run: |
          mkdir -p build/store_arxiv_summaries
          cp services/store_arxiv_summaries/src/requirements.txt build/store_arxiv_summaries/
          cp services/store_arxiv_summaries/src/store_arxiv_summaries/lambda_handler.py build/store_arxiv_summaries/
          pip install -r build/store_arxiv_summaries/requirements.txt -t build/store_arxiv_summaries/

      - name: Build Persist arXiv Summaries Lambda Function
        id: build-persist-arxiv-summaries
        run: |
          mkdir -p build/persist_arxiv_summaries
          cp services/persist_arxiv_summaries/src/requirements.txt build/persist_arxiv_summaries/
          cp services/persist_arxiv_summaries/src/persist_arxiv_summaries/lambda_handler.py build/persist_arxiv_summaries/
          pip install -r build/persist_arxiv_summaries/requirements.txt -t build/persist_arxiv_summaries/

      - name: Ensure S3 bucket exists
        run: |
          if ! aws s3 ls "s3://${{steps.vars.outputs.infra_config_bucket}}" 2>&1 | grep -q 'NoSuchBucket'; then
            echo "Bucket exists."
          else
            echo "Bucket does not exist. Creating bucket..."
            aws s3 mb "s3:/${{steps.vars.outputs.infra_config_bucket}}"
            aws s3api put-bucket-versioning --bucket ${{steps.vars.outputs.infra_config_bucket}} --versioning-configuration Status=Enabled
          fi

      - name: Ensure DynamoDB table exists
        run: |
          TABLE_NAME="${{steps.vars.outputs.backend_dynamodb_table}}"
          REGION="${{env.AWS_REGION}}"
          if aws dynamodb describe-table --table-name $TABLE_NAME 2>&1 | grep -q 'ResourceNotFoundException'; then
            echo "DynamoDB table does not exist. Creating table..."
            aws dynamodb create-table \
              --table-name $TABLE_NAME \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $REGION
            echo "DynamoDB table created."
          else
            echo "DynamoDB table exists."
          fi

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0

      - name: Set Terraform Variables from Environment File
        run: |
          jq -r 'to_entries|map("\(.key)=\(.value|tostring)")|.[]' ${{ env.ENV_FILE }} > env_vars
          while IFS= read -r line; do
            echo "$line" >> $GITHUB_ENV
          done < env_vars

      - name: Initialize Terraform
        run: terraform init -upgrade
        working-directory: ./infra/core

      - name: Validate Terraform
        run: terraform validate
        working-directory: ./infra/core

      - name: Plan Terraform
        id: plan
        run: terraform plan -var-file="${{env.ENV_NAME}}.tfvars"
        working-directory: ./infra/core
        env:
          AWS_DEFAULT_REGION: ${{env.AWS_REGION}}
          TF_VAR_alert_email: ${{secrets.ALERT_EMAIL}}
          TF_VAR_home_ip: ${{secrets.HOME_IP}}
          TF_VAR_neo4j_password: ${{env.NEO4J_CREDS_NEO4J_PASSWORD}}
          TF_VAR_neo4j_username: ${{env.NEO4J_CREDS_NEO4J_USERNAME}}

      - name: Apply Terraform
        run: |
          terraform apply -var-file="${{env.ENV_NAME}}.tfvars" -auto-approve
        working-directory: ./infra/core
        env:
          AWS_DEFAULT_REGION: ${{env.AWS_REGION}}
          TF_VAR_alert_email: ${{secrets.ALERT_EMAIL}}
          TF_VAR_home_ip: ${{secrets.HOME_IP}}
          TF_VAR_neo4j_password: ${{env.NEO4J_CREDS_NEO4J_PASSWORD}}
          TF_VAR_neo4j_username: ${{env.NEO4J_CREDS_NEO4J_USERNAME}}

      - name: Save Terraform Outputs
        run: |
          terraform output -json > terraform_outputs.json
          aws s3 cp terraform_outputs.json s3://${{steps.vars.outputs.infra_config_bucket}}/terraform/${{env.ENV_NAME}}-${{steps.vars.outputs.terraform_outputs_prefix}}.json
        working-directory: ./infra/core

      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform_outputs
          path: ./infra/core/terraform_outputs.json
