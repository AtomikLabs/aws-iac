name: Build

on:
  push:
    branches:
      - main
      - stage
      - dev
    paths:
      - "infra/core/**"
      - ".github/workflows/infra.yaml"
      - "orchestration/**"
  pull_request:
    branches:
      - dev
      - test
    paths:
      - "infra/core/**"
      - ".github/workflows/infra.yaml"
      - "orchestration/**"
  workflow_dispatch:

jobs:
  upload_airflow_files:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Extract variables
        id: vars
        run: |
          echo "app_name=$(grep '^app_name' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "aws_region=$(grep '^aws_region' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT

      - name: Set Region
        run: |
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: copy DAGs to s3
        run: |
          aws s3 sync orchestration/airflow/dags s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/dags --exclude "*.pyc" --exclude "__pycache__/*" --exclude "orchestration/airflow/dags/tests"
          aws s3 sync orchestration/airflow/plugins s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/plugins --exclude "*.pyc" --exclude "__pycache__/*" --exclude "orchestration/airflow/dags/tests"
          aws s3 sync orchestration/airflow/config s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/config --exclude "*.pyc" --exclude "__pycache__/*" --exclude "orchestration/airflow/dags/tests"

      - name: Upload Airflow docker files to s3
        run: |
          aws s3 cp orchestration/airflow/Dockerfile s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/Dockerfile
          aws s3 cp orchestration/airflow/docker-compose.yaml s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/docker-compose.yaml
          aws s3 cp orchestration/airflow/requirements.txt s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/requirements.txt

      - name: Upload Kafka files to s3
        run: |
          aws s3 sync orchestration/kafka s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/kafka --exclude "*.pyc" --exclude "__pycache__/*"

  core_infra:
    runs-on: ubuntu-latest
    needs: upload_airflow_files
    strategy:
      matrix:
        python-version: ["3.10"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Extract variables
        id: vars
        run: |
          echo "app_name=$(grep '^app_name' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "aws_region=$(grep '^aws_region' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT

      - name: Set Region
        run: |
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: Set NEO4J Credentials
        id: neo4j-creds
        run: |
          echo "${{steps.vars.outputs.aws_region}}"
          credentials_arn=""
          if [ "${{ env.ENV_NAME }}" == "dev" ]; then
            credentials_arn="${{ secrets.AWS_DEV_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "prod" ]; then
            credentials_arn="${{ secrets.AWS_PROD_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "stage" ]; then
            credentials_arn="${{ secrets.AWS_STAGE_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "test" ]; then
            credentials_arn="${{ secrets.AWS_TEST_NEO4J_CREDENTIALS }}"
          fi
          echo "NEO4J_CREDENTIALS_ARN=$credentials_arn" >> $GITHUB_OUTPUT

      - name: Get neo4j secrets
        uses: aws-actions/aws-secretsmanager-get-secrets@v1
        with:
          secret-ids: |
            NEO4J_CREDS, ${{ steps.neo4j-creds.outputs.NEO4J_CREDENTIALS_ARN }}
          parse-json-secrets: true

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Ensure S3 bucket exists
        run: |
          if ! aws s3 ls "s3://${{ steps.vars.outputs.infra_config_bucket }}" 2>&1 | grep -q 'NoSuchBucket'; then
            echo "Bucket exists."
          else
            echo "Bucket does not exist. Creating bucket..."
            aws s3 mb "s3:/${{ steps.vars.outputs.infra_config_bucket }}"
            aws s3api put-bucket-versioning --bucket ${{ steps.vars.outputs.infra_config_bucket }} --versioning-configuration Status=Enabled
          fi

      - name: Ensure DynamoDB table exists
        run: |
          TABLE_NAME="${{ steps.vars.outputs.backend_dynamodb_table }}"
          REGION="${{ env.AWS_REGION }}"
          if aws dynamodb describe-table --table-name $TABLE_NAME 2>&1 | grep -q 'ResourceNotFoundException'; then
            echo "DynamoDB table does not exist. Creating table..."
            aws dynamodb create-table \
              --table-name $TABLE_NAME \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $REGION
            echo "DynamoDB table created."
          else
            echo "DynamoDB table exists."
          fi

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0

      - name: Set Terraform Variables from Environment File
        run: |
          jq -r 'to_entries|map("\(.key)=\(.value|tostring)")|.[]' ${{ env.ENV_FILE }} > env_vars
          while IFS= read -r line; do
            echo "$line" >> $GITHUB_ENV
          done < env_vars

      - name: Initialize Terraform
        run: terraform init -upgrade
        working-directory: ./infra/core

      - name: Validate Terraform
        run: terraform validate
        working-directory: ./infra/core

      - name: Plan Terraform
        id: plan
        run: terraform plan -var-file="${{ env.ENV_NAME }}.tfvars"
        working-directory: ./infra/core
        env:
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
          TF_VAR_alert_email: ${{ secrets.ALERT_EMAIL }}
          TF_VAR_home_ip: ${{ secrets.HOME_IP }}
          TF_VAR_neo4j_password: ${{ env.NEO4J_CREDS_NEO4J_PASSWORD }}
          TF_VAR_neo4j_username: ${{ env.NEO4J_CREDS_NEO4J_USERNAME }}

      - name: Apply Terraform
        run: |
          terraform apply -var-file="${{ env.ENV_NAME }}.tfvars" -auto-approve
        working-directory: ./infra/core
        env:
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
          TF_VAR_alert_email: ${{ secrets.ALERT_EMAIL }}
          TF_VAR_home_ip: ${{ secrets.HOME_IP }}
          TF_VAR_neo4j_password: ${{ env.NEO4J_CREDS_NEO4J_PASSWORD }}
          TF_VAR_neo4j_username: ${{ env.NEO4J_CREDS_NEO4J_USERNAME }}

      - name: Save Terraform Outputs
        run: |
          terraform output -json > terraform_outputs.json
          aws s3 cp terraform_outputs.json s3://${{ steps.vars.outputs.infra_config_bucket }}/terraform/${{ env.ENV_NAME }}-${{ steps.vars.outputs.terraform_outputs_prefix }}.json
        working-directory: ./infra/core

      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform_outputs
          path: ./infra/core/terraform_outputs.json

  sync_airflow_instance:
    needs: core_infra
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: Download Terraform Outputs Artifact
        uses: actions/download-artifact@v4
        with:
          name: terraform_outputs
          path: outputs

      - name: Extract variables
        id: vars
        run: |
          echo "airflow_dags_env_path=$(grep '^airflow_dags_env_path' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "app_name=$(grep '^app_name' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "arxiv_api_max_retries=$(grep '^arxiv_api_max_retries' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "arxiv_base_url=$(grep '^arxiv_base_url' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'=' '{print $2}' | xargs)" >> $GITHUB_OUTPUT
          echo "arxiv_ingestion_day_span=$(grep '^arxiv_ingestion_day_span' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "arxiv_sets=$(awk -F'=' '/^arxiv_sets/ {gsub(/ /, "", $2); print $2}' infra/core/${{ env.ENV_NAME }}.tfvars)" >> $GITHUB_OUTPUT
          echo "aws_region=$(grep '^aws_region' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "data_ingestion_key_prefix=$(grep '^data_ingestion_key_prefix' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "fetch_from_arxiv_task_version=$(grep '^fetch_from_arxiv_task_version' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "most_recent_research_records_version=$(grep '^most_recent_research_records_version' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "neo4j_connection_retries=$(grep '^neo4j_connection_retries' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{ env.ENV_NAME }}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT

      - name: Set Region
        run: |
          echo "Setting AWS region for cli commands..."
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Extract Variables from Terraform Outputs
        id: terraform_vars
        run: |
          echo "Extracting instance ID..."
          echo "arxiv_research_ingestion_event_schema=$(jq -r '.arxiv_research_ingestion_event_schema.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT
          echo "aws_glue_registry_name=$(jq -r '.aws_glue_registry_name.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT
          echo "data_arxiv_summaries_ingestion_complete=$(jq -r '.data_arxiv_summaries_ingestion_complete.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT
          echo "data_bucket=$(jq -r '.data_bucket.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT
          echo "instance_id=$(jq -r '.orchestration_instance_id.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT
          echo "neo4j_instance_private_ip=$(jq -r '.neo4j_instance_private_ip.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT
          echo "orchestration_host_private_ip=$(jq -r '.orchestration_host_private_ip.value' outputs/terraform_outputs.json)" >> $GITHUB_OUTPUT

      # single quotes for arxiv_sets to prevent shell expansion of list values
      - name: Create and Upload DAGS Environment File
        id: create_airflow_env_file
        run: |
          echo "Creating DAGS environment file..."
          echo "AIRFLOW_DAGS_ENV_PATH=${{ steps.vars.outputs.airflow_dags_env_path }}" >> .env
          echo "ARXIV_API_MAX_RETRIES=${{ steps.vars.outputs.arxiv_api_max_retries }}" >> .env
          echo "ARXIV_BASE_URL=${{ steps.vars.outputs.arxiv_base_url }}" >> .env
          echo "ARXIV_INGESTION_DAY_SPAN=${{ steps.vars.outputs.arxiv_ingestion_day_span }}" >> .env
          echo "ARXIV_RESEARCH_INGESTION_EVENT_SCHEMA=${{ steps.terraform_vars.outputs.arxiv_research_ingestion_event_schema }}" >> .env
          echo 'ARXIV_SETS=${{ steps.vars.outputs.arxiv_sets }}' >> .env
          echo "AWS_GLUE_REGISTRY_NAME=${{ steps.terraform_vars.outputs.aws_glue_registry_name }}" >> .env
          echo "AWS_REGION=${{ steps.vars.outputs.aws_region }}" >> .env
          echo "DATA_ARXIV_SUMMARIES_INGESTION_COMPLETE_TOPIC=${{ steps.terraform_vars.outputs.data_arxiv_summaries_ingestion_complete }}" >> .env
          echo "DATA_BUCKET=${{ steps.terraform_vars.outputs.data_bucket }}" >> .env
          echo "DATA_INGESTION_KEY_PREFIX=${{ steps.vars.outputs.data_ingestion_key_prefix }}" >> .env
          echo "ENVIRONMENT_NAME=${{ env.ENV_NAME }}" >> .env
          echo "FETCH_FROM_ARXIV_TASK_VERSION=${{ steps.vars.outputs.fetch_from_arxiv_task_version }}" >> .env
          echo "MOST_RECENT_RESEARCH_RECORDS_VERSION=${{ steps.vars.outputs.most_recent_research_records_version }}" >> .env
          echo "NEO4J_CONNECTION_RETRIES=${{ steps.vars.outputs.neo4j_connection_retries }}" >> .env
          echo "NEO4J_URI=neo4j://${{ steps.terraform_vars.outputs.neo4j_instance_private_ip }}:7687" >> .env
          echo "ORCHESTRATION_HOST_PRIVATE_IP=${{ steps.terraform_vars.outputs.orchestration_host_private_ip }}" >> .env
          cat .env
          echo "Copying file to s3..."
          aws s3 cp .env s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/airflow/dags/.env

      - name: Create and Upload Kafka Scripts Environment File
        id: create_kafka_env_file
        run: |
          echo "Creating Kafka scripts environment file..."
          echo "AWS_REGION=${{ steps.vars.outputs.aws_region }}" >> kafka.env
          echo "ENVIRONMENT_NAME=${{ env.ENV_NAME }}" >> kafka.env
          echo "ORCHESTRATION_HOST_PRIVATE_IP=${{ steps.terraform_vars.outputs.orchestration_host_private_ip }}" >> kafka.env
          echo "Copying file to s3..."
          aws s3 cp kafka.env s3://${{ steps.vars.outputs.infra_config_bucket }}/orchestration/${{ env.ENV_NAME }}/kafka/.env

      - name: Send SSM Command to Sync, Rebuild and Restart Airflow
        if: contains(github.event.head_commit.modified, 'orchestration/airflow/requirements.txt')
        run: |
          echo "REBUILDING AIRFLOW..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=instanceids,Values=${{ steps.terraform_vars.outputs.instance_id }}" \
            --parameters '{"commands":["sudo /data/airflow/sync_s3.sh", "sudo docker compose -f /data/airflow/docker-compose.yaml down", "sudo sed -i \\"s/REPLACE_THIS_IP/${{ steps.terraform_vars.outputs.orchestration_host_private_ip }}/g\\" docker-compose.yaml", "sudo docker compose -f /data/airflow/docker-compose.yaml up -d --build"]}' \
            --region ${{ steps.vars.outputs.aws_region }} \
            --output text

      - name: Send SSM Command to Sync and Restart Airflow
        if: ${{ !contains(github.event.head_commit.modified, 'orchestration/airflow/requirements.txt') }}
        run: |
          echo "RESTARTING AIRFLOW..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=instanceids,Values=${{ steps.terraform_vars.outputs.instance_id }}" \
            --parameters "{\"commands\":[\"sudo /data/airflow/sync_s3.sh\", \"sudo docker compose -f /data/airflow/docker-compose.yaml down\", \"sudo sed -i \\\"s/REPLACE_THIS_IP/${{ steps.terraform_vars.outputs.orchestration_host_private_ip }}/g\\\" /data/airflow/docker-compose.yaml\", \"sudo docker compose -f /data/airflow/docker-compose.yaml up -d\"]}" \
            --region ${{ steps.vars.outputs.aws_region }} \
            --output text

      - name: Send SSM Command to Sync and Restart Airflow
        if: ${{ !contains(github.event.head_commit.modified, 'orchestration/airflow/requirements.txt') }}
        run: |
          echo "RESTARTING AIRFLOW..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=instanceids,Values=${{ steps.terraform_vars.outputs.instance_id }}" \
            --parameters "{\"commands\":[\"sudo /data/airflow/sync_s3.sh\", \"sudo docker compose -f /data/airflow/docker-compose.yaml down\", \"sudo sed -i \\\"s/REPLACE_THIS_IP/${{ steps.terraform_vars.outputs.orchestration_host_private_ip }}/g\\\" /data/airflow/docker-compose.yaml\", \"sudo docker compose -f /data/airflow/docker-compose.yaml up -d\"]}" \
            --region ${{ steps.vars.outputs.aws_region }} \
            --output text
