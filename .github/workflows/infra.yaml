name: Build

on:
  push:
    branches:
      - main
      - stage
      - dev
    paths:
      - "infra/core/**"
      - ".github/workflows/infra.yaml"
      - "orchestration/**"
  pull_request:
    branches:
      - dev
      - test
    paths:
      - "infra/core/**"
      - ".github/workflows/infra.yaml"
      - "orchestration/**"
  workflow_dispatch:

jobs:
  upload_airflow_files:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Extract variables
        id: vars
        run: |
          echo "app_name=$(grep '^app_name' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "aws_region=$(grep '^aws_region' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT

      - name: Set Region
        run: |
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: copy DAGs to s3
        run: |
          aws s3 sync orchestration/airflow/dags s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/dags --exclude "*.pyc" --exclude "__pycache__/*" --exclude "*/tests/*"
          aws s3 sync orchestration/airflow/plugins s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/plugins --exclude "*.pyc" --exclude "__pycache__/*" --exclude "*/tests/*"
          aws s3 sync orchestration/airflow/config s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/config --exclude "*.pyc" --exclude "__pycache__/*" --exclude "*/tests/*"

      - name: Upload Airflow docker files to s3
        run: |
          aws s3 cp orchestration/airflow/Dockerfile s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/Dockerfile
          aws s3 cp orchestration/airflow/docker-compose.yaml s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/docker-compose.yaml
          aws s3 cp orchestration/airflow/requirements.txt s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/requirements.txt

  core_infra:
    runs-on: ubuntu-latest
    needs: upload_airflow_files
    strategy:
      matrix:
        python-version: ["3.10"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Extract variables
        id: vars
        run: |
          echo "app_name=$(grep '^app_name' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "aws_region=$(grep '^aws_region' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT
          echo "terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')" >> $GITHUB_OUTPUT

      - name: Set Region
        run: |
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: Set NEO4J Credentials
        id: neo4j-creds
        run: |
          echo "${{steps.vars.outputs.aws_region}}"
          credentials_arn=""
          if [ "${{ env.ENV_NAME }}" == "dev" ]; then
            credentials_arn="${{ secrets.AWS_DEV_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "prod" ]; then
            credentials_arn="${{ secrets.AWS_PROD_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "stage" ]; then
            credentials_arn="${{ secrets.AWS_STAGE_NEO4J_CREDENTIALS }}"
          elif [ "${{ env.ENV_NAME }}" == "test" ]; then
            credentials_arn="${{ secrets.AWS_TEST_NEO4J_CREDENTIALS }}"
          fi
          echo "NEO4J_CREDENTIALS_ARN=$credentials_arn" >> $GITHUB_OUTPUT

      - name: Get neo4j secrets
        uses: aws-actions/aws-secretsmanager-get-secrets@v1
        with:
          secret-ids: |
            NEO4J_CREDS, ${{steps.neo4j-creds.outputs.NEO4J_CREDENTIALS_ARN}}
          parse-json-secrets: true

      - name: Set up Python ${{matrix.python-version}}
        uses: actions/setup-python@v4
        with:
          python-version: ${{matrix.python-version}}

      - name: Build Fetch Daily Summaries Lambda Function
        id: build-fetch-daily-summaries
        run: |
          mkdir -p build/fetch_daily_summaries
          cp services/fetch_daily_summaries/src/requirements.txt build/fetch_daily_summaries/
          cp services/fetch_daily_summaries/src/fetch_daily_summaries/lambda_handler.py build/fetch_daily_summaries/
          pip install -r build/fetch_daily_summaries/requirements.txt -t build/fetch_daily_summaries/

      - name: Build Parse arXiv Summaries Lambda Function
        id: build-parse-arxiv-summaries
        run: |
          mkdir -p build/parse_arxiv_summaries
          cp services/parse_arxiv_summaries/src/requirements.txt build/parse_arxiv_summaries/
          cp services/parse_arxiv_summaries/src/parse_arxiv_summaries/lambda_handler.py build/parse_arxiv_summaries/
          pip install -r build/parse_arxiv_summaries/requirements.txt -t build/parse_arxiv_summaries/

      - name: Build Post arXiv Parse Dispatcher Lambda Function
        id: build-post-arxiv-parse-dispatcher
        run: |
          mkdir -p build/post_arxiv_parse_dispatcher
          cp services/post_arxiv_parse_dispatcher/src/requirements.txt build/post_arxiv_parse_dispatcher/
          cp services/post_arxiv_parse_dispatcher/src/post_arxiv_parse_dispatcher/lambda_handler.py build/post_arxiv_parse_dispatcher/
          pip install -r build/post_arxiv_parse_dispatcher/requirements.txt -t build/post_arxiv_parse_dispatcher/

      - name: Build Store arXiv Summaries Lambda Function
        id: build-store-arxiv-summaries
        run: |
          mkdir -p build/store_arxiv_summaries
          cp services/store_arxiv_summaries/src/requirements.txt build/store_arxiv_summaries/
          cp services/store_arxiv_summaries/src/store_arxiv_summaries/lambda_handler.py build/store_arxiv_summaries/
          pip install -r build/store_arxiv_summaries/requirements.txt -t build/store_arxiv_summaries/

      - name: Build Persist arXiv Summaries Lambda Function
        id: build-persist-arxiv-summaries
        run: |
          mkdir -p build/persist_arxiv_summaries
          cp services/persist_arxiv_summaries/src/requirements.txt build/persist_arxiv_summaries/
          cp services/persist_arxiv_summaries/src/persist_arxiv_summaries/lambda_handler.py build/persist_arxiv_summaries/
          pip install -r build/persist_arxiv_summaries/requirements.txt -t build/persist_arxiv_summaries/

      - name: Ensure S3 bucket exists
        run: |
          if ! aws s3 ls "s3://${{steps.vars.outputs.infra_config_bucket}}" 2>&1 | grep -q 'NoSuchBucket'; then
            echo "Bucket exists."
          else
            echo "Bucket does not exist. Creating bucket..."
            aws s3 mb "s3:/${{steps.vars.outputs.infra_config_bucket}}"
            aws s3api put-bucket-versioning --bucket ${{steps.vars.outputs.infra_config_bucket}} --versioning-configuration Status=Enabled
          fi

      - name: Ensure DynamoDB table exists
        run: |
          TABLE_NAME="${{steps.vars.outputs.backend_dynamodb_table}}"
          REGION="${{env.AWS_REGION}}"
          if aws dynamodb describe-table --table-name $TABLE_NAME 2>&1 | grep -q 'ResourceNotFoundException'; then
            echo "DynamoDB table does not exist. Creating table..."
            aws dynamodb create-table \
              --table-name $TABLE_NAME \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $REGION
            echo "DynamoDB table created."
          else
            echo "DynamoDB table exists."
          fi

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0

      - name: Set Terraform Variables from Environment File
        run: |
          jq -r 'to_entries|map("\(.key)=\(.value|tostring)")|.[]' ${{ env.ENV_FILE }} > env_vars
          while IFS= read -r line; do
            echo "$line" >> $GITHUB_ENV
          done < env_vars

      - name: Initialize Terraform
        run: terraform init -upgrade
        working-directory: ./infra/core

      - name: Validate Terraform
        run: terraform validate
        working-directory: ./infra/core

      - name: Plan Terraform
        id: plan
        run: terraform plan -var-file="${{env.ENV_NAME}}.tfvars"
        working-directory: ./infra/core
        env:
          AWS_DEFAULT_REGION: ${{env.AWS_REGION}}
          TF_VAR_alert_email: ${{secrets.ALERT_EMAIL}}
          TF_VAR_home_ip: ${{secrets.HOME_IP}}
          TF_VAR_neo4j_password: ${{env.NEO4J_CREDS_NEO4J_PASSWORD}}
          TF_VAR_neo4j_username: ${{env.NEO4J_CREDS_NEO4J_USERNAME}}

      - name: Apply Terraform
        run: |
          terraform apply -var-file="${{env.ENV_NAME}}.tfvars" -auto-approve
        working-directory: ./infra/core
        env:
          AWS_DEFAULT_REGION: ${{env.AWS_REGION}}
          TF_VAR_alert_email: ${{secrets.ALERT_EMAIL}}
          TF_VAR_home_ip: ${{secrets.HOME_IP}}
          TF_VAR_neo4j_password: ${{env.NEO4J_CREDS_NEO4J_PASSWORD}}
          TF_VAR_neo4j_username: ${{env.NEO4J_CREDS_NEO4J_USERNAME}}

      - name: Save Terraform Outputs
        run: |
          terraform output -json > terraform_outputs.json
          aws s3 cp terraform_outputs.json s3://${{steps.vars.outputs.infra_config_bucket}}/terraform/${{env.ENV_NAME}}-${{steps.vars.outputs.terraform_outputs_prefix}}.json
        working-directory: ./infra/core

      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform_outputs
          path: ./infra/core/terraform_outputs.json

  sync_airflow_instance:
    needs: core_infra
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get install jq

      - name: Install node and npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install NPM Dependencies
        run: |
          npm install

      - name: Load Environment Variables
        uses: ./.github/actions/load-env-variables

      - name: Extract variables
        id: vars
        run: |
          echo "::set-output app_name=$(grep '^app_name' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')"
          echo "::set-output arxiv_ingestion_day_span=$(grep '^arxiv_ingestion_day_span' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')"
          echo "::set-output aws_region=$(grep '^aws_region' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')"
          echo "::set-output backend_dynamodb_table=$(grep '^backend_dynamodb_table' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')"
          echo "::set-output infra_config_bucket=$(grep '^infra_config_bucket =' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')"
          echo "::set-output terraform_outputs_prefix=$(grep '^terraform_outputs_prefix' infra/core/${{env.ENV_NAME}}.tfvars | awk -F'[= "]+' '{print $2}')"

      - name: Set Region
        run: |
          echo "AWS_REGION=${{steps.vars.outputs.aws_region}}" >> $GITHUB_ENV

      - name: Set AWS Credentials
        uses: ./.github/actions/set-aws-credentials
        with:
          ENVIRONMENT_NAME: ${{ env.ENV_NAME }}
          PROD_AWS_ACCESS_KEY_ID: ${{ secrets.PROD_AWS_ACCESS_KEY_ID }}
          PROD_AWS_SECRET_ACCESS_KEY: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY }}
          STAGE_AWS_ACCESS_KEY_ID: ${{ secrets.STAGE_AWS_ACCESS_KEY_ID }}
          STAGE_AWS_SECRET_ACCESS_KEY: ${{ secrets.STAGE_AWS_SECRET_ACCESS_KEY }}
          DEV_AWS_ACCESS_KEY_ID: ${{ secrets.DEV_AWS_ACCESS_KEY_ID }}
          DEV_AWS_SECRET_ACCESS_KEY: ${{ secrets.DEV_AWS_SECRET_ACCESS_KEY }}

      - name: Download Terraform Outputs Artifact
        uses: actions/download-artifact@v4
        with:
          name: terraform_outputs
          path: outputs

      - name: Create and Upload DAGS Environment File
        id: create_env_file
        run: |
          echo "Creating DAGS environment file..."
          echo "ARXIV_INGESTION_DAY_SPAN=${{steps.vars.outputs.arxiv_ingestion_day_span}}" >> .env
          echo "ENV_NAME=${{ env.ENV_NAME }}" > .env
          echo "Copying file to ec2 /data/airflow/dags..."
          aws s3 cp .env s3://${{steps.vars.outputs.infra_config_bucket}}/orchestration/${{ env.ENV_NAME }}/airflow/dags/.env

      - name: Extract Instance ID from Terraform Outputs
        id: get_instance_id
        run: |
          echo "Extracting instance ID..."
          instance_id=$(jq -r '.orchestration_instance_id.value' outputs/terraform_outputs.json)
          echo "INSTANCE_ID=$instance_id" >> $GITHUB_ENV
          echo "Instance ID: $instance_id"

      - name: Send SSM Command to Sync, Rebuild and Restart Airflow
        if: contains(github.event.head_commit.modified, 'orchestration/airflow/requirements.txt')
        run: |
          echo "REBUILDING AIRFLOW..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=instanceids,Values=${{ env.INSTANCE_ID }}" \
            --parameters '{"commands":["sudo /data/airflow/sync_s3.sh", "sudo docker compose -f /data/airflow/docker-compose.yaml down", "sudo docker compose -f /data/airflow/docker-compose.yaml build", "sudo docker compose -f /data/airflow/docker-compose.yaml up -d"]}' \
            --region ${{ steps.vars.outputs.aws_region }} \
            --output text

      - name: Send SSM Command to Sync and Restart Airflow
        if: ${{ !contains(github.event.head_commit.modified, 'orchestration/airflow/requirements.txt') }}
        run: |
          echo "RESTARTING AIRFLOW..."
          aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --targets "Key=instanceids,Values=${{ env.INSTANCE_ID }}" \
            --parameters '{"commands":["sudo /data/airflow/sync_s3.sh", "sudo docker compose -f /data/airflow/docker-compose.yaml down", "sudo docker compose -f /data/airflow/docker-compose.yaml up -d"]}' \
            --region ${{ steps.vars.outputs.aws_region }} \
            --output text
