{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install python-docx boto3"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLPLOAh938Wl",
    "outputId": "f21a2e5a-fc9b-40c1-ad48-2ace7f3bb53e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.34.14-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
      "Collecting botocore<1.35.0,>=1.34.14 (from boto3)\n",
      "  Downloading botocore-1.34.14-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.14->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.14->boto3) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.14->boto3) (1.16.0)\n",
      "Installing collected packages: python-docx, jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.14 botocore-1.34.14 jmespath-1.0.1 python-docx-1.1.0 s3transfer-0.10.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from html import unescape\n",
    "from xml.etree import ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from docx import Document\n",
    "import os\n",
    "import re\n",
    "import docx\n",
    "from docx.shared import Pt\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import nsdecls\n",
    "from docx.oxml import parse_xml"
   ],
   "metadata": {
    "id": "TERLGVYlthAN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename=\"app.log\", level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ],
   "metadata": {
    "id": "vXUoXrwgvwXv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def fetch_data(base_url: str, from_date: str, set: str) -> list:\n",
    "    full_xml_responses = []\n",
    "    params = {\"verb\": \"ListRecords\", \"set\": set, \"metadataPrefix\": \"oai_dc\", \"from\": from_date}\n",
    "\n",
    "    backoff_times = [30, 120]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            logging.info(f\"Fetching data with parameters: {params}\")\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            full_xml_responses.append(response.text)\n",
    "            print(params)\n",
    "            root = ET.fromstring(response.content)\n",
    "            resumption_token_element = root.find(\".//{http://www.openarchives.org/OAI/2.0/}resumptionToken\")\n",
    "\n",
    "            if resumption_token_element is not None and resumption_token_element.text:\n",
    "                logging.info(f\"Found resumptionToken: {resumption_token_element.text}\")\n",
    "                print(f\"Found resumptionToken: {resumption_token_element.text}\")\n",
    "                time.sleep(5)\n",
    "                params = {\"verb\": \"ListRecords\", \"resumptionToken\": resumption_token_element.text}\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logging.error(f\"HTTP error occurred: {e}\")\n",
    "            logging.error(f\"Response content: {response.text}\")\n",
    "            print(e)\n",
    "\n",
    "            if response.status_code == 503:\n",
    "                backoff_time = response.headers.get(\"Retry-After\", backoff_times.pop(0) if backoff_times else 30)\n",
    "                logging.warning(f\"Received 503 error, backing off for {backoff_time} seconds.\")\n",
    "                print(f\"Received 503 error, backing off for {backoff_time} seconds.\")\n",
    "                time.sleep(int(backoff_time))\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return full_xml_responses"
   ],
   "metadata": {
    "id": "QUS96pu6eSit"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Hardcoded dictionary for category lookup\n",
    "cs_categories_inverted = {\n",
    "    \"Computer Science - Artifical Intelligence\": \"AI\",\n",
    "    \"Computer Science - Hardware Architecture\": \"AR\",\n",
    "    \"Computer Science - Computational Complexity\": \"CC\",\n",
    "    \"Computer Science - Computational Engineering, Finance, and Science\": \"CE\",\n",
    "    \"Computer Science - Computational Geometry\": \"CG\",\n",
    "    \"Computer Science - Computation and Language\": \"CL\",\n",
    "    \"Computer Science - Cryptography and Security\": \"CR\",\n",
    "    \"Computer Science - Computer Vision and Pattern Recognition\": \"CV\",\n",
    "    \"Computer Science - Computers and Society\": \"CY\",\n",
    "    \"Computer Science - Databases\": \"DB\",\n",
    "    \"Computer Science - Distributed, Parallel, and Cluster Computing\": \"DC\",\n",
    "    \"Computer Science - Digital Libraries\": \"DL\",\n",
    "    \"Computer Science - Discrete Mathematics\": \"DM\",\n",
    "    \"Computer Science - Data Structures and Algorithms\": \"DS\",\n",
    "    \"Computer Science - Emerging Technologies\": \"ET\",\n",
    "    \"Computer Science - Formal Languages and Automata Theory\": \"FL\",\n",
    "    \"Computer Science - General Literature\": \"GL\",\n",
    "    \"Computer Science - Graphics\": \"GR\",\n",
    "    \"Computer Science - Computer Science and Game Theory\": \"GT\",\n",
    "    \"Computer Science - Human-Computer Interaction\": \"HC\",\n",
    "    \"Computer Science - Information Retrieval\": \"IR\",\n",
    "    \"Computer Science - Information Theory\": \"IT\",\n",
    "    \"Computer Science - Machine Learning\": \"LG\",\n",
    "    \"Computer Science - Logic in Computer Science\": \"LO\",\n",
    "    \"Computer Science - Multiagent Systems\": \"MA\",\n",
    "    \"Computer Science - Multimedia\": \"MM\",\n",
    "    \"Computer Science - Mathematical Software\": \"MS\",\n",
    "    \"Computer Science - Numerical Analysis\": \"NA\",\n",
    "    \"Computer Science - Neural and Evolutionary Computing\": \"NE\",\n",
    "    \"Computer Science - Networking and Internet Architecture\": \"NI\",\n",
    "    \"Computer Science - Other Computer Science\": \"OH\",\n",
    "    \"Computer Science - Operating Systems\": \"OS\",\n",
    "    \"Computer Science - Performance\": \"PF\",\n",
    "    \"Computer Science - Programming Languages\": \"PL\",\n",
    "    \"Computer Science - Robotics\": \"RO\",\n",
    "    \"Computer Science - Symbolic Computation\": \"SC\",\n",
    "    \"Computer Science - Sound\": \"SD\",\n",
    "    \"Computer Science - Software Engineering\": \"SE\",\n",
    "    \"Computer Science - Social and Information Networks\": \"SI\",\n",
    "    \"Computer Science - Systems and Control\": \"SY\",\n",
    "}\n",
    "\n",
    "\n",
    "def parse_xml_data(xml_data: str, from_date: str) -> dict:\n",
    "    extracted_data_chunk = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xml_data)\n",
    "        ns = {\"oai\": \"http://www.openarchives.org/OAI/2.0/\", \"dc\": \"http://purl.org/dc/elements/1.1/\"}\n",
    "\n",
    "        for record in root.findall(\".//oai:record\", ns):\n",
    "            date_elements = record.findall(\".//dc:date\", ns)\n",
    "            if len(date_elements) != 1:\n",
    "                continue\n",
    "\n",
    "            identifier = record.find(\".//oai:identifier\", ns).text\n",
    "            abstract_url = record.find(\".//dc:identifier\", ns).text\n",
    "\n",
    "            creators_elements = record.findall(\".//dc:creator\", ns)\n",
    "            authors = []\n",
    "            for creator in creators_elements:\n",
    "                name_parts = creator.text.split(\", \", 1)\n",
    "                last_name = name_parts[0]\n",
    "                first_name = name_parts[1] if len(name_parts) > 1 else \"\"\n",
    "                authors.append({\"last_name\": last_name, \"first_name\": first_name})\n",
    "\n",
    "            # Find all subjects\n",
    "            subjects_elements = record.findall(\".//dc:subject\", ns)\n",
    "            categories = [cs_categories_inverted.get(subject.text, \"\") for subject in subjects_elements]\n",
    "\n",
    "            # Primary category is the first one in the list\n",
    "            primary_category = categories[0] if categories else \"\"\n",
    "\n",
    "            abstract = record.find(\".//dc:description\", ns).text\n",
    "            title = record.find(\".//dc:title\", ns).text\n",
    "            date = date_elements[0].text\n",
    "            group = \"cs\"\n",
    "\n",
    "            extracted_data_chunk[\"records\"].append(\n",
    "                {\n",
    "                    \"identifier\": identifier,\n",
    "                    \"abstract_url\": abstract_url,\n",
    "                    \"authors\": authors,\n",
    "                    \"primary_category\": primary_category,\n",
    "                    \"categories\": categories,  # All categories\n",
    "                    \"abstract\": abstract,\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"group\": group,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "\n",
    "    return extracted_data_chunk"
   ],
   "metadata": {
    "id": "gnCYB_HreZL3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def write_to_files(data: list, file_paths: dict) -> None:\n",
    "    \"\"\"Writes the extracted fields to different files.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries containing the extracted fields.\n",
    "        file_paths (dict): A dictionary with field names as keys and file paths as values.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Aggregating all 'records' from each dictionary in the list\n",
    "        aggregated_records = [record for d in data for record in d.get(\"records\", [])]\n",
    "\n",
    "        with open(file_paths[\"records\"], \"w\") as f:\n",
    "            json_data = json.dumps(aggregated_records, indent=4)\n",
    "            f.write(json_data)\n",
    "\n",
    "        logging.info(f\"Successfully wrote records data to {file_paths['records']}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write data: {e}\")"
   ],
   "metadata": {
    "id": "XyHScAnBeetO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def add_hyperlink(paragraph, text, url):\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "    hyperlink = docx.oxml.shared.OxmlElement(\"w:hyperlink\")\n",
    "    hyperlink.set(docx.oxml.shared.qn(\"r:id\"), r_id)\n",
    "    new_run = docx.oxml.shared.OxmlElement(\"w:r\")\n",
    "    rPr = docx.oxml.shared.OxmlElement(\"w:rPr\")\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "    paragraph._p.append(hyperlink)"
   ],
   "metadata": {
    "id": "HQuxQaqyEtDy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def latex_to_human_readable(latex_str):\n",
    "    # Remove $...$ delimiters\n",
    "    latex_str = re.sub(r\"\\$(.*?)\\$\", r\"\\1\", latex_str)\n",
    "\n",
    "    simple_latex_to_text = {\n",
    "        \"\\\\ll\": \"<<\",\n",
    "        \"\\\\alpha\": \"alpha\",\n",
    "        \"\\\\epsilon\": \"epsilon\",\n",
    "        \"\\\\widetilde\": \"widetilde\",\n",
    "        \"\\\\in\": \"in\",\n",
    "        \"\\\\leq\": \"<=\",\n",
    "        \"\\\\geq\": \">=\",\n",
    "        \"\\\\pm\": \"±\",\n",
    "        \"\\\\times\": \"x\",\n",
    "        \"\\\\sim\": \"~\",\n",
    "        \"\\\\approx\": \"≈\",\n",
    "        \"\\\\neq\": \"≠\",\n",
    "        \"\\\\cdot\": \"·\",\n",
    "        \"\\\\ldots\": \"...\",\n",
    "        \"\\\\cdots\": \"...\",\n",
    "        \"\\\\vdots\": \"...\",\n",
    "        \"\\\\ddots\": \"...\",\n",
    "        \"\\\\forall\": \"for all\",\n",
    "        \"\\\\exists\": \"exists\",\n",
    "        \"\\\\nabla\": \"nabla\",\n",
    "        \"\\\\partial\": \"partial\",\n",
    "        \"\\\\{\": \"{\",\n",
    "        \"\\\\}\": \"}\",\n",
    "        \"\\\\:\": \" \",  # Small space\n",
    "        \"\\\\,\": \" \",  # Thin space\n",
    "        \"\\\\;\": \" \",  # Thick space\n",
    "        \"\\\\!\": \"\",  # Negative space\n",
    "        \"_\": \"_\",  # Subscript\n",
    "    }\n",
    "\n",
    "    for latex, text in simple_latex_to_text.items():\n",
    "        latex_str = latex_str.replace(latex, text)\n",
    "\n",
    "    single_arg_pattern = re.compile(r\"\\\\(\\w+){(.*?)}\")\n",
    "    latex_str = single_arg_pattern.sub(r\"\\2\", latex_str)\n",
    "\n",
    "    latex_str = latex_str.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "    latex_str = latex_str.replace(\"--\", \"–\")\n",
    "\n",
    "    return unescape(latex_str)"
   ],
   "metadata": {
    "id": "vwPWeHIHXXRJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_full_show_notes(categories: list, records: list, date: str, group: str):\n",
    "    for category in categories:\n",
    "        doc = Document()\n",
    "        # add intro paragraph gen\n",
    "\n",
    "        for record in records:\n",
    "            if record[\"primary_category\"] == category and record[\"date\"] == date:\n",
    "                title_pdf_paragraph = doc.add_paragraph()\n",
    "\n",
    "                cleaned_title = re.sub(\"\\n\\s*\", \" \", record[\"title\"])\n",
    "                add_hyperlink(title_pdf_paragraph, cleaned_title, record[\"abstract_url\"])\n",
    "\n",
    "                title_pdf_paragraph.add_run(\" [\")\n",
    "                add_hyperlink(title_pdf_paragraph, \"PDF\", record[\"abstract_url\"].replace(\"abs\", \"pdf\"))\n",
    "                title_pdf_paragraph.add_run(\"]\")\n",
    "\n",
    "                authors = [f\"{author['first_name']} {author['last_name']}\" for author in record[\"authors\"]]\n",
    "                doc.add_paragraph(\"by \" + \", \".join(authors))\n",
    "\n",
    "                paragraphs = record[\"abstract\"].split(\"\\n\\n\")\n",
    "                for p in paragraphs:\n",
    "                    cleaned_paragraph = re.sub(\"\\n\\s*\", \" \", p)\n",
    "                    no_latex_paragraph = latex_to_human_readable(cleaned_paragraph)\n",
    "                    doc.add_paragraph(no_latex_paragraph)\n",
    "\n",
    "        file_name = f\"{date}_{group}_{category}_full_show_notes.docx\"\n",
    "        doc.save(os.path.join(\"show_notes\", file_name))"
   ],
   "metadata": {
    "id": "nbwODqjn4POt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_short_show_notes(categories: list, records: list, date: str, group: str):\n",
    "    for category in categories:\n",
    "        doc = Document()\n",
    "        count = 0\n",
    "        for record in records:\n",
    "            if record[\"primary_category\"] == category and record[\"date\"] == date:\n",
    "                count += 1\n",
    "                title_pdf_paragraph = doc.add_paragraph()\n",
    "\n",
    "                cleaned_title = re.sub(\"\\n\\s*\", \" \", record[\"title\"])\n",
    "                add_hyperlink(title_pdf_paragraph, cleaned_title, record[\"abstract_url\"])\n",
    "\n",
    "                title_pdf_paragraph.add_run(\" [\")\n",
    "                add_hyperlink(title_pdf_paragraph, \"PDF\", record[\"abstract_url\"].replace(\"abs\", \"pdf\"))\n",
    "                title_pdf_paragraph.add_run(\"]\")\n",
    "\n",
    "                authors = [f\"{author['first_name']} {author['last_name']}\" for author in record[\"authors\"]]\n",
    "                doc.add_paragraph(\"by \" + \", \".join(authors))\n",
    "                doc.add_paragraph()\n",
    "\n",
    "        print(f\"{category} {count}\")\n",
    "        # file_name = f\"{date}_{group}_{category}_short_show_notes.docx\"\n",
    "        # doc.save(os.path.join('show_notes', file_name))"
   ],
   "metadata": {
    "id": "bnoRcksufuGK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_script(categories, records, date, group):\n",
    "    for category in categories:\n",
    "        doc = Document()\n",
    "        # add intro notes\n",
    "\n",
    "        for record in records:\n",
    "            if record[\"primary_category\"] == category and record[\"date\"] == date:\n",
    "                title_pdf_paragraph = doc.add_paragraph()\n",
    "\n",
    "                cleaned_title = re.sub(\"\\n\\s*\", \" \", record[\"title\"])\n",
    "\n",
    "                title_pdf_paragraph.add_run(cleaned_title)\n",
    "\n",
    "                authors = [f\"{author['first_name']} {author['last_name']}\" for author in record[\"authors\"]]\n",
    "                doc.add_paragraph(\"by \" + \", \".join(authors))\n",
    "                doc.add_paragraph()\n",
    "                paragraphs = record[\"abstract\"].split(\"\\n\\n\")\n",
    "                for p in paragraphs:\n",
    "                    cleaned_paragraph = re.sub(\"\\n\\s*\", \" \", p)\n",
    "                    no_latex_paragraph = latex_to_human_readable(cleaned_paragraph)\n",
    "                    doc.add_paragraph(no_latex_paragraph)\n",
    "\n",
    "                doc.add_paragraph()\n",
    "\n",
    "        file_name = f\"{date}_{group}_{category}_script.docx\"\n",
    "        doc.save(os.path.join(\"show_notes\", file_name))"
   ],
   "metadata": {
    "id": "nuumgzt8n7Gz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def insert_into_database(data: dict, db_config: dict) -> None:\n",
    "    \"\"\"Inserts the extracted fields into a database.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary containing the extracted fields.\n",
    "        db_config (dict): Database configuration details including host, user, password, etc.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\""
   ],
   "metadata": {
    "id": "eEBC1bgaeNml"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "FROM_DATE = \"2024-01-03\"\n",
    "BASE_URL = \"http://export.arxiv.org/oai2\"\n",
    "logging.info(\"Application started.\")\n",
    "xml_data_list = fetch_data(BASE_URL, FROM_DATE, \"cs\")"
   ],
   "metadata": {
    "id": "VcXpxUCMeGAC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b952eed-00b8-49d2-8f24-39157b2cbe00"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'verb': 'ListRecords', 'set': 'cs', 'metadataPrefix': 'oai_dc', 'from': '2024-01-03'}\n",
      "Found resumptionToken: 6933954|1001\n",
      "{'verb': 'ListRecords', 'resumptionToken': '6933954|1001'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "extracted_data = []\n",
    "\n",
    "for xml_data in xml_data_list:\n",
    "    extracted_data.append(parse_xml_data(xml_data, FROM_DATE))"
   ],
   "metadata": {
    "id": "HrlY_COrfawX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(extracted_data))\n",
    "print(extracted_data[1][\"records\"][0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-VtpZlhgVgJ",
    "outputId": "341ec193-fe0c-433f-ce69-fbe88ecd18dc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n",
      "{'identifier': 'oai:arXiv.org:2401.01851', 'abstract_url': 'http://arxiv.org/abs/2401.01851', 'authors': [{'last_name': 'Geißler', 'first_name': 'Daniel'}, {'last_name': 'Zhou', 'first_name': 'Bo'}, {'last_name': 'Liu', 'first_name': 'Mengxi'}, {'last_name': 'Suh', 'first_name': 'Sungho'}, {'last_name': 'Lukowicz', 'first_name': 'Paul'}], 'primary_category': 'LG', 'categories': ['LG', '', 'PF'], 'abstract': '  This work examines the effects of variations in machine learning training\\nregimes and learning paradigms on the corresponding energy consumption. While\\nincreasing data availability and innovation in high-performance hardware fuels\\nthe training of sophisticated models, it also supports the fading perception of\\nenergy consumption and carbon emission. Therefore, the goal of this work is to\\ncreate awareness about the energy impact of general training parameters and\\nprocesses, from learning rate over batch size to knowledge transfer. Multiple\\nsetups with different hyperparameter initializations are evaluated on two\\ndifferent hardware configurations to obtain meaningful results. Experiments on\\npretraining and multitask training are conducted on top of the baseline results\\nto determine their potential towards sustainable machine learning.\\n', 'title': 'The Power of Training: How Different Neural Network Setups Influence the\\n  Energy Demand', 'date': '2024-01-03', 'group': 'cs'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "FILE_PATHS = {\"records\": \"records.json\"}\n",
    "write_to_files(extracted_data, FILE_PATHS)"
   ],
   "metadata": {
    "id": "elAC6rrufdYw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "records = [record for data in extracted_data for record in data.get(\"records\", [])]"
   ],
   "metadata": {
    "id": "gqkaBXncEI2Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "create_full_show_notes([\"CL\", \"CV\", \"RO\"], records, \"2024-01-05\", \"cs\")"
   ],
   "metadata": {
    "id": "iNQ8Rpur4Vwf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "create_script([\"CL\", \"CV\", \"RO\"], records, \"2024-01-04\", \"cs\")"
   ],
   "metadata": {
    "id": "Do28N4jQpAV3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "create_short_show_notes([\"CV\"], records, \"2023-11-30\", \"cs\")"
   ],
   "metadata": {
    "id": "3EmMDk4ifxMx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "780bc28f-88cb-4258-f213-81b0825369be"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CV 0\n"
     ]
    }
   ]
  }
 ]
}